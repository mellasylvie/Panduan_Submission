# -*- coding: utf-8 -*-
"""Predictive Analytics Prediksi Harga Diamond.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ODol5-oqWQvh6CR9Z8kaQkV0Mnf4DZLB

# **Proyek Machine Learning - Prediksi Harga Berlian (Diamond)**

---

**Mellania Permata**

**Submission Machine Learning Terapan - Predictive Analytics**

## **Pendahuluan**

Proyek ini bertujuan untuk membangun **model predictive analytics** yang dapat memprediksi harga berlian (diamond) berdasarkan karakteristik utamanya, seperti karat, warna, kejernihan, potongan, dan dimensi fisik lainnya. Harga berlian sangat dipengaruhi oleh kombinasi atribut tersebut, dan kesalahan dalam penilaian harga dapat merugikan pihak penjual maupun pembeli.

## **Import Library**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""## **1. Data Understanding**

Pada bagian ini, akan dilakukan load dataset dan Eksplorasi Dataset (EDA)

### 1.1. Data Loading
"""

# load the dataset
url = 'https://raw.githubusercontent.com/tidyverse/ggplot2/master/data-raw/diamonds.csv'
diamonds = pd.read_csv(url)
diamonds

# Melihat informasi data
diamonds.info()

"""**Penjelasan variabel dataset :**     

- Harga dalam dolar Amerika Serikat ($) adalah fitur target.
- carat: merepresentasikan bobot (weight) dari diamonds (0.2-5.01), digunakan sebagai ukuran dari batu permata dan perhiasan.
- cut: merepresentasikan kualitas pemotongan diamonds (Fair, Good, Very Good, Premium, and Ideal).
- color: merepresentasikan warna, dari J (paling buruk) ke D (yang terbaik).
- clarity: merepresentasikan seberapa jernih diamonds (I1 (paling buruk), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (terbaik))
- x: merepresentasikan panjang diamonds dalam mm (0-10.74).
- y: merepresentasikan lebar diamonds dalam mm (0-58.9).
- z: merepresentasikan kedalaman diamonds dalam mm (0-31.8).
- depth: merepresentasikan z/mean(x, y) = 2 * z/(x + y) (43-79).
- table: merepresentasikan lebar bagian atas berlian relatif terhadap titik terlebar 43-95).
"""

#Deskripsi Statistik Data
diamonds.describe()

"""#### Memeriksa Missing Value"""

x = (diamonds.x == 0).sum()
y = (diamonds.y == 0).sum()
z = (diamonds.z == 0).sum()

print("Nilai 0 di kolom x ada: ", x)
print("Nilai 0 di kolom y ada: ", y)
print("Nilai 0 di kolom z ada: ", z)

"""Pada kolom x ditemukan 8 data dengan nilai 0, Pada kolom y ditemukan 7 data dengan nilai 0, dan Pada kolom z ditemukan 20 data dengan nilai 0."""

diamonds.loc[(diamonds['z']==0)]

"""#### Memeriksa Outlier"""

def cek_outlier_boxplot(diamonds, cols_per_row):
    df_numerik = diamonds.select_dtypes(include=['float64', 'int64'])
    kolom = df_numerik.columns
    jumlah_kolom = len(kolom)

    # Hitung jumlah baris untuk grid
    baris = int(np.ceil(jumlah_kolom / cols_per_row))

    # Buat figure dan axes
    fig, axes = plt.subplots(baris, cols_per_row, figsize=(5 * cols_per_row, 4 * baris))
    axes = axes.flatten()

    # Loop dan buat boxplot
    for i, col in enumerate(kolom):
        sns.boxplot(x=df_numerik[col], ax=axes[i], color='lightblue')
        axes[i].set_title(f'Boxplot {col}')

    # Matikan subplot kosong jika jumlah kolom tidak pas dengan grid
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

cek_outlier_boxplot(diamonds, 3)

"""#### Memeriksa Data Duplikat"""

cek_duplikat = diamonds.duplicated().sum()
print("Jumlah data duplikat :", cek_duplikat)

"""### 1.2. Exploratory Data Analysis

Pada Explorasi Data Analysis akan dilakukan 2 Analisa, yaitu :    
1. Univariate Analysis
2. Multivariate Analysis

#### 1.2.1 Univariate Analysis

Mengelompokkan fitur
"""

numerical_features = ['price', 'carat', 'depth', 'table', 'x', 'y', 'z']
categorical_features = ['cut', 'color', 'clarity']

"""##### Analisa Fitur Cut"""

feature = categorical_features[0]
count = diamonds[feature].value_counts()
percent = 100*diamonds[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Terdapat 5 kategori pada fitur Cut, secara berurutan dari jumlahnya yang paling banyak yaitu: Ideal, Premium, Very Good, Good, dan Fair. Dari data persentase dapat kita simpulkan bahwa lebih dari 60% sampel merupakan diamonds tipe grade tinggi, yaitu grade Ideal dan Premium

##### Analisis Fitur Color
"""

feature = categorical_features[1]
count = diamonds[feature].value_counts()
percent = 100*diamonds[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Berdasarkan deskripsi variabel, urutan kategori warna dari yang paling buruk ke yang paling bagus adalah J, I, H, G, F, E, dan D. Dari grafik di atas, dapat kita simpulkan bahwa sebagian besar grade berada pada grade menengah, yaitu G, F, H.

##### Analisis Fitur Clarity
"""

feature = categorical_features[2]
count = diamonds[feature].value_counts()
percent = 100*diamonds[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature)

"""Berdasarkan informasi dari deskripsi variabel, fitur Clarity terdiri dari 8 kategori dari yang paling buruk ke yang paling baik, yaitu: I1, SI2, SI1, VS2, VS1, VVS2, VVS1, dan IF

##### Analisis Numerikal Feature
"""

diamonds.hist(bins=50, figsize=(20,15))
plt.show()

"""**Insight :**

Mari amati histogram di atas, khususnya histogram untuk variabel "price" yang merupakan fitur target (label) pada data kita. Dari histogram "price", kita bisa memperoleh beberapa informasi, antara lain:

- Peningkatan harga diamonds sebanding dengan penurunan jumlah sampel. Hal ini dapat kita lihat jelas dari histogram "price" yang grafiknya mengalami penurunan seiring dengan semakin banyaknya jumlah sampel (sumbu y).
- Rentang harga diamonds cukup tinggi yaitu dari skala ratusan dolar Amerika hingga sekitar $11800

- Setengah harga berlian bernilai di bawah $2500.
- Distribusi harga miring ke kanan (right-skewed). Hal ini akan berimplikasi pada model.

#### 1.2.2. Multivariate Analysis

##### Analisis Categorical Feature
"""

cat_features = diamonds.select_dtypes(include='object').columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y="price", kind="bar", dodge=False, height = 4, aspect = 3,  data=diamonds, palette="Set3")
  plt.title("Rata-rata 'price' Relatif terhadap - {}".format(col))

"""**Insight**

- Pada fitur ‘cut’, rata-rata harga cenderung mirip. Rentangnya berada antara 3500 hingga 4500. Grade tertinggi yaitu grade Ideal memiliki harga rata-rata terendah diantara grade lainnya. Sehingga, fitur cut memiliki pengaruh atau dampak yang kecil terhadap rata-rata harga.
- Pada fitur ‘color’, semakin rendah grade warna, harga diamonds justru semakin tinggi. Dari sini dapat disimpulkan bahwa warna memiliki pengaruh yang rendah terhadap harga.
- Pada fitur ‘clarity’, secara umum, diamond dengan grade lebih rendah memiliki harga yang lebih tinggi. Hal ini berarti bahwa fitur ‘clarity’ memiliki pengaruh yang rendah terhadap harga.
- Kesimpulan akhir, fitur kategori memiliki pengaruh yang rendah terhadap harga.

##### Analisis Numerikal Feature (Multivariate Analysis)
"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(diamonds, diag_kind = 'kde')

"""##### Correlation Matrix"""

plt.figure(figsize=(10, 8))
correlation_matrix = diamonds[numerical_features].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""**Insight :**

fitur ‘carat’, ‘x, ‘y’, dan ‘z’ memiliki skor korelasi yang besar (di atas 0.9) dengan fitur target ‘price’. Artinya, fitur 'price' berkorelasi tinggi dengan keempat fitur tersebut. Sementara itu, fitur ‘depth’ memiliki korelasi yang sangat kecil (0.01). Sehingga, fitur tersebut dapat di-drop.

## **2. Data Preparation**

### 2.1. Menangani Missing Value
"""

# Drop baris dengan nilai 'x', 'y', dan 'z' = 0
diamonds = diamonds.loc[(diamonds[['x','y','z']]!=0).all(axis=1)]

#cek kembali missing value
diamonds.isnull().sum()

"""### 2.2. Menangani Outlier"""

# Ambil hanya kolom numerikal
numeric_cols = diamonds.select_dtypes(include='number').columns
# Hitung Q1, Q3, dan IQR hanya untuk kolom numerikal
Q1 = diamonds[numeric_cols].quantile(0.25)
Q3 = diamonds[numeric_cols].quantile(0.75)
IQR = Q3 - Q1
# Buat filter untuk menghapus baris yang mengandung outlier di kolom numerikal
filter_outliers = ~((diamonds[numeric_cols] < (Q1 - 1.5 * IQR)) |
                    (diamonds[numeric_cols] > (Q3 + 1.5 * IQR))).any(axis=1)
# Terapkan filter ke dataset asli (termasuk kolom non-numerikal)
diamonds = diamonds[filter_outliers]

# Cek ukuran dataset setelah outlier dihapus
diamonds.shape

# Melihat kembali outlier
def cek_outlier_boxplot(diamonds, cols_per_row):
    df_numerik = diamonds.select_dtypes(include=['float64', 'int64'])
    kolom = df_numerik.columns
    jumlah_kolom = len(kolom)

    # Hitung jumlah baris untuk grid
    baris = int(np.ceil(jumlah_kolom / cols_per_row))

    # Buat figure dan axes
    fig, axes = plt.subplots(baris, cols_per_row, figsize=(5 * cols_per_row, 4 * baris))
    axes = axes.flatten()

    # Loop dan buat boxplot
    for i, col in enumerate(kolom):
        sns.boxplot(x=df_numerik[col], ax=axes[i], color='lightblue')
        axes[i].set_title(f'Boxplot {col}')

    # Matikan subplot kosong jika jumlah kolom tidak pas dengan grid
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

cek_outlier_boxplot(diamonds, 3)

"""### 2.3. Menangani Data Duplikat"""

diamonds.drop_duplicates()

"""### 2.4. Encoding Fitur Categorical"""

from sklearn.preprocessing import  OneHotEncoder
diamonds = pd.concat([diamonds, pd.get_dummies(diamonds['cut'], prefix='cut')],axis=1)
diamonds = pd.concat([diamonds, pd.get_dummies(diamonds['color'], prefix='color')],axis=1)
diamonds = pd.concat([diamonds, pd.get_dummies(diamonds['clarity'], prefix='clarity')],axis=1)
diamonds.drop(['cut','color','clarity'], axis=1, inplace=True)
diamonds.head()

"""### 2.5. Reduksi Dimensi PCA"""

from sklearn.decomposition import PCA

pca = PCA(n_components=3, random_state=123)
pca.fit(diamonds[['x','y','z']])
princ_comp = pca.transform(diamonds[['x','y','z']])

pca.explained_variance_ratio_.round(3)

pca = PCA(n_components=1, random_state=123)
pca.fit(diamonds[['x','y','z']])
diamonds['dimension'] = pca.transform(diamonds.loc[:, ('x','y','z')]).flatten()
diamonds.drop(['x','y','z'], axis=1, inplace=True)

"""### 2.6. Pembagian Data"""

from sklearn.model_selection import train_test_split

X = diamonds.drop(["price"],axis =1)
y = diamonds["price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

# Cek jumlah data
print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""### 2.7. Standarisasi Data"""

from sklearn.preprocessing import StandardScaler

numerical_features = ['carat', 'table', 'dimension']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

"""## **3. Modeling**

Pada tahap ini, kita akan mengembangkan model machine learning dengan tiga algoritma. Kemudian, kita akan mengevaluasi performa masing-masing algoritma dan menentukan algoritma mana yang memberikan hasil prediksi terbaik. Ketiga algoritma yang akan kita gunakan, antara lain:

1. K-Nearest Neighbor
2. Random Forest
3. Boosting Algorithm
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""### 3.1. Model KNN"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train)

models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

"""Berikut nilai parameter yang digunakan pada pemodelan diatas :    
- n_neighbors = 10

### 3.2. Model Random Forest
"""

# Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestRegressor

# buat model prediksi
RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)

models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

"""Berikut nilai parameter yang digunakan pada pemodelan diatas :

- n_estimators = 50
- max_depth = 16
- random_state = 55
- n_jobs = -1

### 3.3. Model Boosting (Adaboost)
"""

from sklearn.ensemble import AdaBoostRegressor

boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)
boosting.fit(X_train, y_train)
models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

"""Berikut nilai parameter yang digunakan pada pemodelan diatas :

learning_rate = 0.05
random_state = 55

## **4. Evaluasi**

Metrik yang akan kita gunakan pada prediksi ini adalah MSE atau Mean Squared Error yang menghitung jumlah selisih kuadrat rata-rata nilai sebenarnya dengan nilai prediksi
"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

# Panggil mse
mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""**Hasil Evaluasi :**    
Dari gambar di atas, terlihat bahwa, model Random Forest (RF) memberikan nilai eror yang paling kecil. Sedangkan model dengan algoritma Boosting memiliki eror yang paling besar (berdasarkan grafik, angkanya di atas 800). Sehingga model RF yang akan kita pilih sebagai model terbaik untuk melakukan prediksi harga diamonds.

**Prediksi**
"""

prediksi = X_test.iloc[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)

"""Terlihat bahwa prediksi dengan Random Forest (RF) memberikan hasil yang paling mendekati."""